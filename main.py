import os
import json
import re
import docx
from sentence_transformers import SentenceTransformer

# ==============================
# –ù–∞—Å—Ç—Ä–æ–π–∫–∏
# ==============================
DOCX_FILE = "Dop_materialy_AI_pomoshhnik_po_mediam_0a34958fc5.docx"
JSON_OUTPUT = "events_vector_db.json"
MODEL_NAME = "cointegrated/LaBSE-en-ru"  # ‚Üê –±–æ–ª–µ–µ —Ç–æ—á–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞

# ==============================
# –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–æ–±—ã—Ç–∏–π –∏–∑ .docx
# ==============================
def extract_events_precise(file_path):
    if not os.path.isfile(file_path):
        raise FileNotFoundError(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")

    doc = docx.Document(file_path)
    lines = [para.text for para in doc.paragraphs if para.text.strip() != ""]

    MONTHS = [
        '—è–Ω–≤–∞—Ä[—å—è]', '—Ñ–µ–≤—Ä–∞–ª[—å—è]', '–º–∞—Ä—Ç–∞?', '–∞–ø—Ä–µ–ª[—å—è]', '–º–∞[–π—è]', '–∏—é–Ω[—å—è]',
        '–∏—é–ª[—å—è]', '–∞–≤–≥—É—Å—Ç–∞?', '—Å–µ–Ω—Ç—è–±—Ä[—å—è]', '–æ–∫—Ç—è–±—Ä[—å—è]', '–Ω–æ—è–±—Ä[—å—è]', '–¥–µ–∫–∞–±—Ä[—å—è]'
    ]
    MONTH_PATTERN = '|'.join(MONTHS)

    date_pattern = re.compile(
        r'^\s*('
        r'\d{1,2}[./]\s*\d{1,2}(?:[./]\s*\d{2,4})?|'
        r'\d{1,2}\s*[‚Äì\-]\s*\d{1,2}\s+(?:' + MONTH_PATTERN + r')|'
        r'\d{1,2}\s+(?:' + MONTH_PATTERN + r')(?:\s+\d{4})?|'
        r'(?:' + MONTH_PATTERN + r')\s+\d{4}|'
        r'\d{1,2}[./]\d{1,2}\s*[‚Äì\-]\s*\d{1,2}[./]\d{1,2}|'
        r'\d{1,2}\s*[‚Äì\-]\s*\d{1,2}\s*(?:' + MONTH_PATTERN + r')'
        r')',
        re.IGNORECASE | re.UNICODE
    )

    year_pattern = re.compile(r'^\s*(2024|2025)\s*$', re.UNICODE)
    section_header_pattern = re.compile(r'^\s*–û–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω–∞—è –ø–æ–≤–µ—Å—Ç–∫–∞,\s*\d{4}\s*$', re.IGNORECASE | re.UNICODE)

    events = []
    current_date = None
    current_lines = []
    current_year = None

    for line in lines:
        stripped = line.strip()
        if not stripped:
            continue

        if section_header_pattern.match(stripped):
            continue

        year_match = year_pattern.match(stripped)
        if year_match:
            current_year = year_match.group(1)
            continue

        date_match = date_pattern.match(stripped)
        if date_match:
            if current_date is not None and current_lines:
                full_desc = ' '.join(current_lines).strip()
                events.append((current_date, full_desc))
                current_lines = []

            date_str = date_match.group(1).strip()
            desc_part = stripped[len(date_match.group(0)):].strip()
            desc_part = re.sub(r'^[-‚Äì‚Äî:\s]+', '', desc_part)

            if current_year and not re.search(r'\b20\d{2}\b', date_str):
                if re.search(r'[–∞-—è—ë]', date_str, re.IGNORECASE):
                    date_str += f" {current_year}"
                elif '.' in date_str:
                    date_str += f".{current_year[2:]}"
                elif '/' in date_str:
                    date_str += f"/{current_year[2:]}"
                else:
                    date_str += f" {current_year}"

            current_date = date_str
            if desc_part:
                current_lines.append(desc_part)
        else:
            if current_date is not None:
                current_lines.append(stripped)

    if current_date is not None and current_lines:
        events.append((current_date, ' '.join(current_lines).strip()))

    return events

# ==============================
# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
# ==============================
def clean_event_for_embedding(text: str) -> str:
    # –£–±–∏—Ä–∞–µ–º —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤ ‚Äî –æ–Ω–∏ –º–µ—à–∞—é—Ç —Å–µ–º–∞–Ω—Ç–∏–∫–µ
    text = re.sub(r'\b\d+\s*(—É—á–∞—Å—Ç–Ω–∏–∫|—á–µ–ª–æ–≤–µ–∫|–ø–µ—Ä—Å–æ–Ω|—á–µ–ª\.?)\b', '', text, flags=re.IGNORECASE)
    # –ë–µ—Ä—ë–º —Ç–æ–ª—å–∫–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫ –¥–æ –ø–µ—Ä–≤–æ–≥–æ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è
    parts = re.split(r'[‚Äì‚Äî:]', text, maxsplit=1)
    core = parts[0].strip()
    if len(core.split()) < 2:
        core = text.strip()
    return re.sub(r'\s+', ' ', core)

# ==============================
# –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è
# ==============================
def main():
    print("üîç –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–æ–±—ã—Ç–∏–π –∏–∑ DOCX...")
    events = extract_events_precise(DOCX_FILE)
    print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(events)} —Å–æ–±—ã—Ç–∏–π.")

    print(f"üß† –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏: {MODEL_NAME} (–Ω–∞ CPU)...")
    model = SentenceTransformer(MODEL_NAME, device="cpu")  # ‚Üê –±–µ–∑–æ–ø–∞—Å–Ω–æ –¥–ª—è GTX 940MX

    print("üî¢ –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Å–æ–±—ã—Ç–∏–π...")
    database = []
    for i, (date_str, raw_text) in enumerate(events):
        clean_text = clean_event_for_embedding(raw_text)
        vector = model.encode(clean_text, convert_to_numpy=True, normalize_embeddings=True)
        database.append({
            "date": date_str,
            "text": raw_text,
            "vector": vector.tolist()
        })
        if (i + 1) % 10 == 0 or i == len(events) - 1:
            print(f"  [{i+1}/{len(events)}] {clean_text[:60]}...")

    print("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã...")
    with open(JSON_OUTPUT, "w", encoding="utf-8") as f:
        json.dump(database, f, ensure_ascii=False, indent=2)

    print(f"‚úÖ –ë–∞–∑–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {JSON_OUTPUT}")
    print(f"üìä –†–∞–∑–º–µ—Ä –≤–µ–∫—Ç–æ—Ä–∞: {vector.shape[0]} (–¥–æ–ª–∂–µ–Ω –±—ã—Ç—å 768)")

if __name__ == "__main__":
    main()